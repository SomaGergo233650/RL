{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Goal Position: [ 0.18919028 -0.07907485  0.19176464]\n",
      "Step 1/2000:\n",
      "  Current Position: [0.0725 0.0889 0.1205]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 208.27 mm\n",
      "  Accumulated Reward: -5.2083\n",
      "Step 101/2000:\n",
      "  Current Position: [ 0.0304 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.93 mm\n",
      "  Accumulated Reward: 674.4350\n",
      "Step 201/2000:\n",
      "  Current Position: [ 0.0305 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.94 mm\n",
      "  Accumulated Reward: 937.1441\n",
      "Step 301/2000:\n",
      "  Current Position: [ 0.0305 -0.035   0.2551]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.84 mm\n",
      "  Accumulated Reward: 1244.8535\n",
      "Step 401/2000:\n",
      "  Current Position: [ 0.0305 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.94 mm\n",
      "  Accumulated Reward: 1522.5627\n",
      "Step 501/2000:\n",
      "  Current Position: [ 0.0305 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 1860.2721\n",
      "Step 601/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 2137.9814\n",
      "Step 701/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 2370.6907\n",
      "Step 801/2000:\n",
      "  Current Position: [ 0.0304 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.93 mm\n",
      "  Accumulated Reward: 2603.4001\n",
      "Step 901/2000:\n",
      "  Current Position: [ 0.0304 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.93 mm\n",
      "  Accumulated Reward: 2791.1093\n",
      "Step 1001/2000:\n",
      "  Current Position: [ 0.0305 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 3023.8187\n",
      "Step 1101/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.2551]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.81 mm\n",
      "  Accumulated Reward: 3286.5280\n",
      "Step 1201/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 3504.2372\n",
      "Step 1301/2000:\n",
      "  Current Position: [ 0.0304 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.93 mm\n",
      "  Accumulated Reward: 3721.9466\n",
      "Step 1401/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 3924.6559\n",
      "Step 1501/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.2551]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.81 mm\n",
      "  Accumulated Reward: 4247.3652\n",
      "Step 1601/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 4510.0745\n",
      "Step 1701/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 4787.7838\n",
      "Step 1801/2000:\n",
      "  Current Position: [ 0.0305 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.94 mm\n",
      "  Accumulated Reward: 4945.4931\n",
      "Step 1901/2000:\n",
      "  Current Position: [ 0.0304 -0.0351  0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.91 mm\n",
      "  Accumulated Reward: 5208.2025\n",
      "Step 2000/2000:\n",
      "  Current Position: [ 0.0304 -0.035   0.255 ]\n",
      "  Goal Position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "  Distance to Goal: 22.93 mm\n",
      "  Accumulated Reward: 5445.9347\n",
      "\n",
      "Benchmark Results:\n",
      "total_steps: 2000\n",
      "total_rewards: 5445.934747362509\n",
      "distance_per_step: [208.27192068099976, 205.96909523010254, 202.52211391925812, 197.84317910671234, 192.53532588481903, 184.72589552402496, 177.50537395477295, 171.01366817951202, 165.0485098361969, 158.96472334861755] ...\n",
      "total_time: 36.79059934616089\n",
      "goal_position: [ 0.02913594 -0.04158256  0.27693227]\n",
      "final_position: [ 0.0304 -0.035   0.255 ]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sim_class import Simulation\n",
    "from stable_baselines3 import PPO\n",
    "import time\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, render=False, max_steps=1000):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.render_enabled = render\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Create the simulation environment\n",
    "        self.sim = Simulation(num_agents=1, render=self.render_enabled)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1, -1, 0]), high=np.array([1, 1, 1, 1]), shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "        # Track steps and goal position\n",
    "        self.steps = 0\n",
    "        self.goal_position = None\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Set a random goal position within the valid range\n",
    "        self.goal_position = np.array([\n",
    "            np.random.uniform(-0.18700, 0.25300),\n",
    "            np.random.uniform(-0.17050, 0.21950),\n",
    "            np.random.uniform(0.16940, 0.28950)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # Reset the simulation environment\n",
    "        observation = self.sim.reset(num_agents=1)\n",
    "\n",
    "        # Extract pipette position and combine with goal position\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.prev_distance = np.linalg.norm(pipette_position - self.goal_position)  # Initialize prev_distance\n",
    "        return observation, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Extract current pipette position from the environment\n",
    "        observation = self.sim.run([action])\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "\n",
    "        # Update observation\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "\n",
    "        # Calculate distance to the goal\n",
    "        distance = np.linalg.norm(pipette_position - self.goal_position)\n",
    "\n",
    "        # Calculate reward based on distance to the goal\n",
    "        reward = -distance  # Base reward\n",
    "        if distance <= 0.001:  # If the agent reaches the goal\n",
    "            reward += 100  # Large positive reward for achieving the goal\n",
    "        elif self.steps > 0 and self.prev_distance > distance:  # If the agent is moving closer to the goal\n",
    "            reward += 10  # Additional reward for getting closer\n",
    "        else:  # If the agent is moving away from the goal\n",
    "            reward -= 5  # Penalty for moving further away\n",
    "\n",
    "        # Update the previous distance\n",
    "        self.prev_distance = distance\n",
    "\n",
    "        # Check termination condition (10 mm accuracy requirement)\n",
    "        terminated = distance <= 0.001  # 10 mm accuracy\n",
    "\n",
    "        # Check truncation\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        self.steps += 1\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_enabled:\n",
    "            self.sim.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.sim.close()\n",
    "\n",
    "def benchmark(goal_position=None, max_steps=2000, position_change_threshold=1e-3, max_stuck_steps=2000):\n",
    "    \"\"\"\n",
    "    Run the benchmark for the RL controller and detect if the position doesn't change for max_stuck_steps.\n",
    "\n",
    "    :param goal_position: The goal position to reach.\n",
    "    :param max_steps: Maximum steps for the benchmark.\n",
    "    :param position_change_threshold: Threshold to detect when position has not changed.\n",
    "    :param max_stuck_steps: Maximum number of consecutive steps where the position doesn't change.\n",
    "    :return: Dictionary with benchmark results.\n",
    "    \"\"\"\n",
    "    env = CustomEnv(render=True)\n",
    "    model = PPO.load(\"model.zip\")  # Load the trained PPO model\n",
    "\n",
    "    # If no goal position is provided, generate a random one\n",
    "    if goal_position is None:\n",
    "        goal_position = np.array([\n",
    "            np.random.uniform(-0.18700, 0.25300),\n",
    "            np.random.uniform(-0.17050, 0.21950),\n",
    "            np.random.uniform(0.16940, 0.28950)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    print(f\"Random Goal Position: {goal_position}\")\n",
    "\n",
    "    # Reset the environment with the goal position\n",
    "    observation, _ = env.reset()\n",
    "    goal_position = observation[3:]  # The goal position is in the last 3 values of the observation\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    rewards = []\n",
    "    distances = []\n",
    "    steps = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    prev_position = observation[:3]\n",
    "    stuck_steps = 0\n",
    "\n",
    "    for step in range(max_steps):  # Simulate up to max_steps\n",
    "        # Predict the action from the trained model\n",
    "        action, _ = model.predict(observation, deterministic=True)  # Use deterministic=True for testing\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Calculate the accuracy (distance to the goal) in mm\n",
    "        current_position = observation[:3]  # Extract the current position from the observation\n",
    "        distance_to_goal = np.linalg.norm(goal_position - current_position) * 1000  # Convert to mm\n",
    "\n",
    "        # Store the metrics for benchmarking\n",
    "        rewards.append(reward)\n",
    "        distances.append(distance_to_goal)\n",
    "        steps.append(step + 1)  # Store the step count (1-based indexing)\n",
    "\n",
    "        # Print debug information for every 100 steps\n",
    "        if step % 100 == 0 or step == max_steps - 1:\n",
    "            print(f\"Step {step + 1}/{max_steps}:\")\n",
    "            print(f\"  Current Position: {current_position}\")\n",
    "            print(f\"  Goal Position: {goal_position}\")\n",
    "            print(f\"  Distance to Goal: {distance_to_goal:.2f} mm\")\n",
    "            print(f\"  Accumulated Reward: {sum(rewards):.4f}\")\n",
    "\n",
    "        # Check if position has changed significantly\n",
    "        if np.linalg.norm(current_position - prev_position) < position_change_threshold:\n",
    "            stuck_steps += 1\n",
    "        else:\n",
    "            stuck_steps = 0\n",
    "\n",
    "        # If the position hasn't changed for max_stuck_steps, consider it as truncated\n",
    "        if stuck_steps >= max_stuck_steps:\n",
    "            print(\"Position hasn't changed significantly for too long. Continuing...\")\n",
    "            stuck_steps = 0  # Reset stuck_steps to allow continuation\n",
    "\n",
    "        # Update previous position for the next step\n",
    "        prev_position = current_position\n",
    "\n",
    "    # Calculate the total time taken to reach the goal\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Collect results\n",
    "    results = {\n",
    "        'total_steps': len(steps),\n",
    "        'total_rewards': sum(rewards),\n",
    "        'distance_per_step': distances,\n",
    "        'total_time': total_time,\n",
    "        'goal_position': goal_position,\n",
    "        'final_position': current_position,\n",
    "    }\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Testing Process\n",
    "if __name__ == \"__main__\":\n",
    "    results = benchmark(max_steps=2000)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"{key}: {value[:10]} ...\")  # Display first 10 values for long lists\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
